{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split",
      "execution_count": 88,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def drop_out(layer_data, m, keep_prob):\n    np.random.seed(2)\n    D = {}\n    L = len(layer_data)\n    for i in range(L):\n        D[str(i)] = np.random.rand(layer_data[i], m)\n        D[str(i)] = D[str(i)]<keep_prob[i]\n    return D",
      "execution_count": 101,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def sigmoid(z):\n    A = 1/(1+np.exp(-z))\n    cache = z                      #cache is helpful to keep track in backprop\n    return A,cache\n\ndef relu(z):\n    A = np.maximum(0,z)\n    cache = z\n    return A,cache\n\ndef tanh(z):\n    A = np.tanh(z)\n    cache = z\n    return A,cache",
      "execution_count": 90,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def back_sigmoid(dA, cache):              #calculates derivative for backprop\n    z = cache\n    s = 1/(1+np.exp(-z))\n    dz = dA*s*(1-s)\n    return dz\n\ndef back_relu(dA, cache):\n    z = cache\n    dz = np.array(dA, copy=True)    \n    dz[z<=0] = 0                    #set values to zero which are negative\n    return dz\n\ndef back_tanh(dA, cache):\n    z = cache\n    s = np.tanh(z)\n    dz = dA*(1-np.square(s))\n    return dz",
      "execution_count": 91,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def ini_params_deep(layer_data):\n    np.random.seed(1)\n    params = {}\n    L = len(layer_data)       #no. of layers for finding how many w and b\n    for i in range(1,L):\n        params['w'+str(i)] = ((np.random.randn(layer_data[i],layer_data[i-1]))/np.sqrt(layer_data[i-1]))  #initialize randomly \n        params['b'+str(i)] = (np.zeros((layer_data[i],1)))               #set b to 0 initially\n    return params",
      "execution_count": 92,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def lin_fwd(A,w,b):\n    Z = w.dot(A) + b           #the linear caclculation part\n    cache = (A,w,b)            #store the result which helps in backprop\n    return Z,cache\n\ndef lin_act_fwd(A_last,w,b,act):          #activation functions\n    if act=='sigmoid':\n        Z,lin_cache = lin_fwd(A_last,w,b)\n        A,act_cache = sigmoid(Z)\n    elif act=='relu':\n        Z,lin_cache = lin_fwd(A_last,w,b)\n        A,act_cache = relu(Z)\n    elif act=='tanh':\n        Z,lin_cache = lin_fwd(A_last,w,b)\n        A,act_cache = tanh(Z)\n     \n    cache = (lin_cache,act_cache)           #store, helpful during backprop\n    return A, cache\n\ndef L_model_fwd_drop(X,params,D,keep_prob):            #integrates all the above for the calculation of forward propagation\n    caches = []\n    A = X\n    A = np.multiply(A, D[str(0)])\n    A /= keep_prob[0]\n    L = len(params)//2           #calculate the no. of layers, divide by 2 because of w and b\n    for i in range(1,L):\n        A_last = A\n        A, cache = lin_act_fwd(A_last, params['w'+str(i)], params['b'+str(i)], 'relu')      #call linear function\n        A = np.multiply(A, D[str(i)])\n        A /= keep_prob[i]\n        caches.append(cache)\n    Al, cache = lin_act_fwd(A, params['w'+str(L)], params['b'+str(L)], 'sigmoid')       #call the activation\n    Al = np.multiply(Al, D[str(L)])\n    Al /= keep_prob[L]\n    caches.append(cache)                         #caches keep store of the values, useful for backprop\n    return Al, caches\n\ndef L_model_fwd(X,params):            #integrates all the above for the calculation of forward propagation\n    caches = []\n    A = X\n    L = len(params)//2           #calculate the no. of layers, divide by 2 because of w and b\n    for i in range(1,L):\n        A_last = A\n        A, cache = lin_act_fwd(A_last, params['w'+str(i)], params['b'+str(i)], 'relu')      #call linear function\n        caches.append(cache)\n    Al, cache = lin_act_fwd(A, params['w'+str(L)], params['b'+str(L)], 'sigmoid')       #call the activation\n    caches.append(cache)                         #caches keep store of the values, useful for backprop\n    return Al, caches",
      "execution_count": 93,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def cost(Al,Y):\n    m = Y.shape[1]\n    cost = (-np.dot(Y,np.log(Al).T) - np.dot((1-Y),np.log(1-Al).T))/m        #the cost function for binary classifier \n    return cost",
      "execution_count": 94,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def lin_back(dz, cache):                 #part of backprop that finds the derivatives\n    A_last, w, b = cache\n    m = A_last.shape[1]\n    dw = (1./m)*np.dot(dz,A_last.T)                     #derivative for w\n    db = (1./m)*np.sum(dz, axis=1, keepdims=True)       #derivative for b\n    dA_last = np.dot(w.T,dz)                           #derivative of A or activation\n    return dA_last, dw, db\n\ndef lin_act_back(dA, cache, act):                #calls the various derivative functions\n    lin_cache, act_cache = cache                #recalls the two caches from cache\n    if act == 'relu':\n        dz = back_relu(dA, act_cache)\n        dA_last, dw, db = lin_back(dz, lin_cache)\n    elif act == 'sigmoid':\n        dz = back_sigmoid(dA, act_cache)\n        dA_last, dw, db = lin_back(dz, lin_cache)\n    elif act == 'tanh':\n        dz = back_tanh(dA, act_cache)\n        dA_last, dw, db = lin_back(dz, lin_cache)\n    return dA_last, dw, db\n\ndef L_model_back(Al, Y, caches,D,keep_prob):                       #the main backward propagation\n    grads = {}\n    L = len(caches)\n    m = Al.shape[1]\n    Y = Y.reshape(Al.shape)\n    dAl = -(np.divide(Y,Al) - np.divide((1-Y),(1-Al)))        #derivative for final layer\n    dAl = np.multiply(dAl, D[str(L)])\n    dAl /= keep_prob[L]\n    curr_cache = caches[L-1]\n    grads['dA'+str(L)], grads['dw'+str(L)], grads['db'+str(L)] = lin_act_back(dAl, curr_cache, 'sigmoid')\n    grads['dA'+str(L)] = np.multiply(grads['dA'+str(L)], D[str(L)])\n    grads['dA'+str(L)] /= keep_prob[L]\n    for i in reversed(range(L-1)):                           #start in reversed for backward prop\n        curr_cache = caches[i]\n        dA_last_temp, dw_temp, db_temp = lin_act_back(grads['dA'+str(i+2)], curr_cache, 'relu')\n        grads['dA'+str(i+1)] = dA_last_temp\n        grads['dA'+str(i+2)] = np.multiply(grads['dA'+str(i+2)], D[str(i+2)])\n        grads['dA'+str(i+2)] /= keep_prob[i+2]\n        grads['dw'+str(i+1)] = dw_temp\n        grads['db'+str(i+1)] = db_temp\n    return grads                                #grads contains dw, db, and dA\n\ndef update(params, grads, lr):              #lr is learning rate\n    L = len(params)//2\n    for i in range(L):\n        params['w'+str(i+1)] = params['w'+str(i+1)] - lr*grads['dw'+str(i+1)]        #update rule for w\n        params['b'+str(i+1)] = params['b'+str(i+1)] - lr*grads['db'+str(i+1)]       #update rule for b\n    return params",
      "execution_count": 113,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def predict(X, y, params):                         #the prediction function for test set\n    m = X.shape[1]\n    #n = len(params)\n    p = np.zeros((1,m))                            #the predicted output\n    probs, caches = L_model_fwd(X, params)\n    for i in range(0, probs.shape[1]):\n        if probs[0,i]>0.5:\n            p[0,i] = 1\n        else:\n            p[0,i] = 0\n    return p",
      "execution_count": 114,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def L_layer(X, Y, layer_data, keep_prob, lr, num_iter, print_cost=False ):   #train the neural network\n    np.random.seed(3)\n    m = X.shape[1]\n    costs = []\n    params = ini_params_deep(layer_data)\n    for i in range(0, num_iter):\n        D = drop_out(layer_data, m, keep_prob)\n        Al, caches = L_model_fwd_drop(X, params, D, keep_prob)\n        cost_compute = cost(Al,Y)\n        grads = L_model_back(Al, Y, caches, D, keep_prob)\n        params = update(params, grads, lr)\n        if (print_cost and i%100) == 0:\n            print('Cost after iteration %i: %f' %(i, cost_compute))       #print the value of costs after 100 iterations\n        if (print_cost and i%100) == 0:\n            costs.append(cost_compute)\n    plt.plot(np.squeeze(costs))                    #plot the costs vs. iterations\n    plt.ylabel('cost')\n    plt.xlabel('iterations(per tens)')\n    plt.title('learning rate = '+str(lr))\n    plt.show()\n    return params",
      "execution_count": 115,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def data():\n    data_orig = pd.read_csv('housepricedata.csv')           #read data in pandas dataframe\n    X_orig = data_orig.iloc[:,0:10].values                  #convert to numpy array\n    m = X_orig.shape[0]\n    Y_orig = data_orig.iloc[:,10].values.reshape(m,1)\n    X_train, X_test, Y_train, Y_test = train_test_split(X_orig, Y_orig, test_size=0.2)       #randomly split into 80-20 train-test set \n    scale = preprocessing.StandardScaler().fit(X_train)\n    X_train_new = scale.transform(X_train)                #after processing the data using skLearn library\n    X_test_new = scale.transform(X_test)                  #for getting a standard data range for all the features\n    X = X_train_new.T\n    Y = Y_train.T\n    X_t = X_test_new.T\n    Y_t = Y_test.T\n    n_x = X.shape[0]                  #input nodes\n    n_y = Y.shape[0]                #output nodes\n    layer_data = [n_x,25, 25, n_y]         #specifies how many layers and how many nodes in each layer\n    return X, Y, X_t, Y_t, layer_data",
      "execution_count": 116,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": "X, Y, X_t, Y_t, layer_data = data()        #get the data\n\nprint(layer_data)                         #prints the no. of layers and hidden nodes in each\nkeep_prob = [1,1,0.5,1]\n\nparams = L_layer(X,Y,layer_data,keep_prob,0.005,5000,True)         #train the data and find the parameters\n\np = predict(X_t, Y_t, params)                         #find the predicted output on the test set\nTP = np.sum(np.logical_and(p==1,Y_t==1))              #true positive\nTN = np.sum(np.logical_and(p==0,Y_t==0))              #true negative\nFP = np.sum(np.logical_and(p==1,Y_t==0))              #false positive\nFN = np.sum(np.logical_and(p==0,Y_t==1))              #false negative\n\nP=TP/(TP+FP)                                          #precision\nR=TP/(TP+FN)                                          #recall\nF=(2*P*R)/(P+R)                                       #F-score\naccuracy = (TP+TN)/(TP + TN + FP + FN)                #accuracy\nprint(\"The F-score: %f and accuracy: %f\" %(F,accuracy))",
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n  warnings.warn(msg, DataConversionWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n  warnings.warn(msg, DataConversionWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n  warnings.warn(msg, DataConversionWarning)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "[10, 25, 25, 1]\nCost after iteration 0: 0.868856\nCost after iteration 100: 0.769541\nCost after iteration 200: 0.711470\nCost after iteration 300: 0.665667\nCost after iteration 400: 0.625377\nCost after iteration 500: 0.588460\nCost after iteration 600: 0.553333\nCost after iteration 700: 0.519918\nCost after iteration 800: 0.488609\nCost after iteration 900: 0.459666\nCost after iteration 1000: 0.433832\nCost after iteration 1100: 0.411356\nCost after iteration 1200: 0.392427\nCost after iteration 1300: 0.376662\nCost after iteration 1400: 0.363551\nCost after iteration 1500: 0.352620\nCost after iteration 1600: 0.343546\nCost after iteration 1700: 0.335890\nCost after iteration 1800: 0.329301\nCost after iteration 1900: 0.323598\nCost after iteration 2000: 0.318587\nCost after iteration 2100: 0.314134\nCost after iteration 2200: 0.310053\nCost after iteration 2300: 0.306252\nCost after iteration 2400: 0.302709\nCost after iteration 2500: 0.299397\nCost after iteration 2600: 0.296272\nCost after iteration 2700: 0.293308\nCost after iteration 2800: 0.290467\nCost after iteration 2900: 0.287715\nCost after iteration 3000: 0.285043\nCost after iteration 3100: 0.282494\nCost after iteration 3200: 0.280077\nCost after iteration 3300: 0.277768\nCost after iteration 3400: 0.275532\nCost after iteration 3500: 0.273395\nCost after iteration 3600: 0.271365\nCost after iteration 3700: 0.269421\nCost after iteration 3800: 0.267569\nCost after iteration 3900: 0.265822\nCost after iteration 4000: 0.264147\nCost after iteration 4100: 0.262534\nCost after iteration 4200: 0.260988\nCost after iteration 4300: 0.259503\nCost after iteration 4400: 0.258076\nCost after iteration 4500: 0.256735\nCost after iteration 4600: 0.255461\nCost after iteration 4700: 0.254239\nCost after iteration 4800: 0.253068\nCost after iteration 4900: 0.251965\n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": "<Figure size 432x288 with 1 Axes>"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": "The F-score: 0.879121 and accuracy: 0.886986\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "celltoolbar": "Raw Cell Format"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}