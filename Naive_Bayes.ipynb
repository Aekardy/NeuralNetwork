{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from collections import defaultdict as dd\nimport pandas as pd\nimport numpy as np\nimport time\n",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Reading the dataset\n\ncols=[\"sentence\", \"sentiment\"]\nraw_df=pd.read_csv(\"a1_d3.txt\", sep='\\t', names=cols, dtype={\"sentence\":\"str\", \"sentiment\":\"int\"})\n\nraw_df[\"sentence\"]=raw_df[\"sentence\"].str.lower()",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class SimpleClassifier(object):\n    \n    def __init__(self, n_gram=1, printing=False):\n        self.prior = dd(int)\n        self.n = n_gram\n        self.logprior = {}\n        self.count_of_words = dd(list)\n        self.loglikelihoods = dd(dd)\n        self.V = []\n        \n#Computing prior and word count\n\n    def CPWC(self, training_set, training_labels):\n        for x, y in zip(training_set['sentence'], training_labels['sentiment']):\n            all_words = x.split(\" \")\n            if self.n == 1:\n                grams = all_words\n            else:\n                grams = self.words_to_grams(all_words)\n\n            self.prior[y] += len(grams)\n            self.count_of_words[y].append(x)\n            \n#Computing word in classes\n\n    def CWIC(self):\n        count = {}\n        for c in list(self.count_of_words.keys()):\n            docs = self.count_of_words[c]\n            count[c] = dd(int)\n            for doc in docs:\n                words = doc.split(\" \")\n                for word in words:\n                    count[c][word] += 1\n\n        return count\n\n#Computing vocabulary\n\n    def CV(self, documents):\n        vocabulary = set()\n\n        for doc in documents:\n            for word in doc.split(\" \"):\n                vocabulary.add(word.lower())\n\n        return vocabulary\n\n\n#Training the classifier\n\n    def train(self, N_c, training_set, training_labels, alpha=1):\n\n        # For getting number of documents\n\n        N_doc = len(training_set)\n\n        # For getting vocabulary used in training set\n\n        self.V = self.CV(training_set['sentence'])\n\n        # Create count_of_words for computing the word count\n\n        for x, y in zip(training_set['sentence'], training_labels['sentiment']):\n            self.count_of_words[y].append(x)\n\n        # Get set of all classes\n\n        all_classes = set(training_labels['sentiment'])\n\n        # All word count for each class are computed in a dictionary\n\n        self.count_of_words = self.CWIC()\n\n        # Computing all the count necessary to compute the two terms of the reformulated\n\n        for c in all_classes:\n            if(c==0):\n                N_c=float(N_c)\n            else:\n                N_c=float(N_doc-N_c)\n                \n            # Computing logprior for class\n\n            self.logprior[c] = np.log(N_c / N_doc)\n\n            # Calculate the sum of count of words in current class\n\n            total_count = 0\n            for word in self.V:\n                total_count += self.count_of_words[c][word]\n\n            # For every word, get the counting and compute the log-likelihood for this class\n\n            for word in self.V:\n                counting = self.count_of_words[c][word]\n                self.loglikelihoods[c][word] = np.log((counting + alpha) / (total_count + alpha * len(self.V)))\n\n    def predicting(self, test_doc):\n        sums = {\n            0: 0,\n            1: 0,\n        }\n        for c in self.count_of_words.keys():\n            sums[c] = self.logprior[c]\n            words = test_doc.split(\" \")\n            for word in words:\n               if word in self.V:\n                   sums[c] += self.loglikelihoods[c][word]\n\n        return sums",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def prediction_evaluator(validation_set,validation_labels,trained_classifier):\n  correct_predictions = 0\n  predictions_list = []\n  prediction = -1\n  \n  TP=0\n  FP=0\n  TN=0 \n  FN=0\n\n  for dataset,label in zip(validation_set['sentence'], validation_labels['sentiment']):\n    probabilities = trained_classifier.predicting(dataset)\n\n    #Comparing probabilities\n\n    if probabilities[0] >= probabilities[1]:\n      prediction = 0\n    elif  probabilities[0] < probabilities[1]:\n      prediction = 1\n\n    #Checking if the predicted value is same as actual one\n\n    if prediction == label:\n      correct_predictions += 1\n      predictions_list.append(\"correct\")    #If prediction same then append \"correct\" else append \"wrong\"\n    else:\n      predictions_list.append(\"wrong\")\n    \n    #True positives\n    if prediction == label and prediction ==1:\n        TP+=1\n    #True negatives\n    if prediction == label and prediction ==0:\n        TN+=1\n    #False positives\n    if prediction != label and prediction ==1:\n        FP+=1\n    #False negatives\n    if prediction != label and prediction ==0:\n        FN+=1\n    \n  precision=TP/(TP+FP)\n  recall=TP/(TP+FN)\n  F_score=2*precision*recall/(precision+recall)\n    \n  print(\"Correct Predictions: {} out of {} ({}%)\".format(correct_predictions,len(validation_labels),round(correct_predictions/len(validation_labels)*100,5)))\n  return predictions_list, round(correct_predictions/len(validation_labels)*100), F_score\n\nn_folds=5\nF_Score_list=[]\naccuracy_list=[]",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Time for splitting data and computation\n\nstart = time.time()\n\n#splitting into n folds\n\nfor i in range(n_folds):\n    \n    msk = np.empty(len(raw_df), dtype=bool)\n    np.array(msk)\n\n#Boolean numpy array for splitting\n\n    for j in range(len(raw_df)):\n        if j>=200*i and j<200*(i+1):\n            msk[j]=False\n        else:\n            msk[j]=True\n\n#Making datasets with test size of 200 for 5 folds\n\n    train=raw_df[msk]\n    test=raw_df[~msk]\n\n#Splitting into lines and their sentiments\n\n    training_set=train.iloc[:, [0]]\n    training_labels=train.iloc[:, [1]]\n    validation_set=test.iloc[:, [0]]\n    validation_labels=test.iloc[:, [1]]\n    \n    N_c=0\n\n#Counting number of negative sentiments\n\n    for t in training_labels['sentiment']:\n        if t==0:\n            N_c=N_c+1\n            \n    NaiveBayesClassifier = SimpleClassifier()\n    NaiveBayesClassifier.train(N_c, training_set, training_labels, alpha=1)\n    prediction_list, accuracy, F_score = prediction_evaluator(validation_set, validation_labels, NaiveBayesClassifier)\n    print(\"F-score is\" , F_score, \"\\n\")\n    accuracy_list.append(accuracy)\n    F_Score_list.append((F_score))\n    \n\n\nprint(\"\\nF-Score is {}±{}\".format(round(np.mean(F_Score_list),4),round(np.std(F_Score_list),4)))\nprint(\"Accuracy is {}±{}\\n\".format(round(np.mean(accuracy_list),2),round(np.std(accuracy_list),2)))    \n            \nend = time.time()\nprint('Ran in {} seconds'.format(round(end - start, 3)))",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Correct Predictions: 159 out of 200 (79.5%)\nF-score is 0.8075117370892019 \n\nCorrect Predictions: 159 out of 200 (79.5%)\nF-score is 0.8093023255813954 \n\nCorrect Predictions: 151 out of 200 (75.5%)\nF-score is 0.7741935483870969 \n\nCorrect Predictions: 154 out of 200 (77.0%)\nF-score is 0.7653061224489796 \n\nCorrect Predictions: 154 out of 200 (77.0%)\nF-score is 0.7553191489361702 \n\n\nF-Score is 0.7823±0.0221\nAccuracy is 78.0±1.67\n\nRan in 0.271 seconds\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}